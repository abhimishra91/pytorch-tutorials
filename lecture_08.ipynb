{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 08:\n",
    "\n",
    "* **DataLoader** used to load the data to the model in batches. This is done because not all the data can be loaded to the network in 1 go. \n",
    "\n",
    "\n",
    "- **1 Epoch** is the passing of all the data through the network once. \n",
    "- **Batch Size** is the minimum amout of data that will pass through the network\n",
    "- **Iterations** No. of passes of data to be made to cover the complete training data. \n",
    "\n",
    "\n",
    "_For a training data of `1000 rows` and batch size of `500` the iteration is `2`_\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for creating the dataloader\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    # Initiatize your data, download process etc\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv', delimiter=',', skiprows=1, dtype = np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:,0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:,-1])\n",
    "        \n",
    "    # This will return the element from the data, based on the index value    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    # Return the length of the data\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "\n",
    "# Object for MyDataset class\n",
    "dataset = MyDataset()\n",
    "\n",
    "# Creating the loader\n",
    "train_loader = DataLoader(dataset = dataset, batch_size = 32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch network and training\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the network with the layers\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8,6)\n",
    "        self.l2 = torch.nn.Linear(6,8)\n",
    "        self.l3 = torch.nn.Linear(8,1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defining the forward pass of the data based on the layers created above\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# Creating Model object\n",
    "model = Model()\n",
    "\n",
    "# Loss function called criterion. This is the binary corss entropy.\n",
    "# Also create the optimizer to update the gradient. Using the SGD here.\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimus = torch.optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6850202679634094\n",
      "Epoch: 0, Loss:  0.7063078284263611\n",
      "Epoch: 0, Loss:  0.7715358138084412\n",
      "Epoch: 0, Loss:  0.7151461839675903\n",
      "Epoch: 0, Loss:  0.7090287208557129\n",
      "Epoch: 0, Loss:  0.6874685287475586\n",
      "Epoch: 0, Loss:  0.686786949634552\n",
      "Epoch: 0, Loss:  0.680263102054596\n",
      "Epoch: 0, Loss:  0.6956996321678162\n",
      "Epoch: 0, Loss:  0.662934422492981\n",
      "Epoch: 0, Loss:  0.6796655654907227\n",
      "Epoch: 0, Loss:  0.672856330871582\n",
      "Epoch: 0, Loss:  0.6612431406974792\n",
      "Epoch: 0, Loss:  0.6768894791603088\n",
      "Epoch: 0, Loss:  0.6287937164306641\n",
      "Epoch: 0, Loss:  0.6391973495483398\n",
      "Epoch: 0, Loss:  0.6554039120674133\n",
      "Epoch: 0, Loss:  0.6606767177581787\n",
      "Epoch: 0, Loss:  0.6127085089683533\n",
      "Epoch: 0, Loss:  0.6212042570114136\n",
      "Epoch: 0, Loss:  0.6492659449577332\n",
      "Epoch: 0, Loss:  0.6614421010017395\n",
      "Epoch: 0, Loss:  0.6139333844184875\n",
      "Epoch: 0, Loss:  0.6641162633895874\n",
      "Epoch: 1, Loss:  0.6617295145988464\n",
      "Epoch: 1, Loss:  0.6448819637298584\n",
      "Epoch: 1, Loss:  0.5934086441993713\n",
      "Epoch: 1, Loss:  0.6472995281219482\n",
      "Epoch: 1, Loss:  0.7720071077346802\n",
      "Epoch: 1, Loss:  0.7074853777885437\n",
      "Epoch: 1, Loss:  0.647002100944519\n",
      "Epoch: 1, Loss:  0.6644510626792908\n",
      "Epoch: 1, Loss:  0.6477363109588623\n",
      "Epoch: 1, Loss:  0.6591359972953796\n",
      "Epoch: 1, Loss:  0.6448429822921753\n",
      "Epoch: 1, Loss:  0.6117604374885559\n",
      "Epoch: 1, Loss:  0.6275312304496765\n",
      "Epoch: 1, Loss:  0.6068578958511353\n",
      "Epoch: 1, Loss:  0.548807680606842\n",
      "Epoch: 1, Loss:  0.7080508470535278\n",
      "Epoch: 1, Loss:  0.6454963088035583\n",
      "Epoch: 1, Loss:  0.6226770281791687\n",
      "Epoch: 1, Loss:  0.6826448440551758\n",
      "Epoch: 1, Loss:  0.587816596031189\n",
      "Epoch: 1, Loss:  0.6265673041343689\n",
      "Epoch: 1, Loss:  0.7271870970726013\n",
      "Epoch: 1, Loss:  0.6611766219139099\n",
      "Epoch: 1, Loss:  0.6052882671356201\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # getting the inputs and labels\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # forward pass\n",
    "        y_pred = model(inputs)\n",
    "        \n",
    "        # Loss and print\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        # Make optimizer items zero, back propagation \n",
    "        # Then updating the wegihts using step\n",
    "        optimus.zero_grad()\n",
    "        loss.backward()\n",
    "        optimus.step()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
