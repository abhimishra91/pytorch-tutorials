{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_df = pd.read_csv('./data/iris.csv', delimiter=',')\n",
    "x_data = xy_df.iloc[:,0:-1].to_numpy()\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pd.get_dummies(xy_df.iloc[:,-1]).values.argmax(1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Network\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the network with the layers\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(4, 10)\n",
    "        self.l2 = torch.nn.Linear(10, 8)\n",
    "        self.l3 = torch.nn.Linear(8, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defining the forward pass of the data based on the layers created above\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Dataset and Dataloader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        xy_df = pd.read_csv('./data/iris.csv', delimiter=',')\n",
    "        self.len = xy_df.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy_df.iloc[:,0:-1].to_numpy(dtype=np.float32))\n",
    "        self.y_data = torch.from_numpy((pd.get_dummies(xy_df.iloc[:,-1])).values.argmax(1).astype(np.float32))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "my_dataset = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = my_dataset, batch_size=16, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimus = torch.optim.SGD(model.parameters(), lr = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5405045747756958\n",
      "Epoch: 0, Loss: 0.4360932409763336\n",
      "Epoch: 0, Loss: 0.5628055930137634\n",
      "Epoch: 0, Loss: 0.41424405574798584\n",
      "Epoch: 0, Loss: 0.3848228454589844\n",
      "Epoch: 0, Loss: 0.2688494324684143\n",
      "Epoch: 0, Loss: 0.4241332709789276\n",
      "Epoch: 0, Loss: 0.5201851725578308\n",
      "Epoch: 0, Loss: 0.3002222776412964\n",
      "Epoch: 0, Loss: 0.3375948369503021\n",
      "Epoch: 1, Loss: 0.07833589613437653\n",
      "Epoch: 1, Loss: 0.29924270510673523\n",
      "Epoch: 1, Loss: 0.5644407272338867\n",
      "Epoch: 1, Loss: 0.2867211103439331\n",
      "Epoch: 1, Loss: 0.059754449874162674\n",
      "Epoch: 1, Loss: 0.48803412914276123\n",
      "Epoch: 1, Loss: 0.02433396875858307\n",
      "Epoch: 1, Loss: 0.07684843987226486\n",
      "Epoch: 1, Loss: 0.6535183191299438\n",
      "Epoch: 1, Loss: 0.23174096643924713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # getting the inputs and labels\n",
    "        inputs, labels = data\n",
    "#         print(f'labels: {labels.shape}')\n",
    "        # Predictions from the model\n",
    "        y_pred = model(inputs)\n",
    "#         print(labels.shape)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "        \n",
    "        # optimus roll out\n",
    "        optimus.zero_grad()\n",
    "        loss.backward()\n",
    "        optimus.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
