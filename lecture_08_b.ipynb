{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitfastaiconda149f4ca18fae45818735beadf08062d0",
   "display_name": "Python 3.7.6 64-bit ('fastai': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice File with Wine Dataset\n",
    "\n",
    "The result is shit. To be further improved with Softmax implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataloader and dataset library\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(4898, 12)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Analyzing the data real quick to see how it is structured\n",
    "\n",
    "wine = pd.read_csv('./data/winequality-white.csv')\n",
    "wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.0              0.27         0.36            20.7      0.045   \n1            6.3              0.30         0.34             1.6      0.049   \n2            8.1              0.28         0.40             6.9      0.050   \n3            7.2              0.23         0.32             8.5      0.058   \n4            7.2              0.23         0.32             8.5      0.058   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 45.0                 170.0   1.0010  3.00       0.45   \n1                 14.0                 132.0   0.9940  3.30       0.49   \n2                 30.0                  97.0   0.9951  3.26       0.44   \n3                 47.0                 186.0   0.9956  3.19       0.40   \n4                 47.0                 186.0   0.9956  3.19       0.40   \n\n   alcohol  quality  \n0      8.8        6  \n1      9.5        6  \n2     10.1        6  \n3      9.9        6  \n4      9.9        6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.0</td>\n      <td>0.27</td>\n      <td>0.36</td>\n      <td>20.7</td>\n      <td>0.045</td>\n      <td>45.0</td>\n      <td>170.0</td>\n      <td>1.0010</td>\n      <td>3.00</td>\n      <td>0.45</td>\n      <td>8.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.3</td>\n      <td>0.30</td>\n      <td>0.34</td>\n      <td>1.6</td>\n      <td>0.049</td>\n      <td>14.0</td>\n      <td>132.0</td>\n      <td>0.9940</td>\n      <td>3.30</td>\n      <td>0.49</td>\n      <td>9.5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.1</td>\n      <td>0.28</td>\n      <td>0.40</td>\n      <td>6.9</td>\n      <td>0.050</td>\n      <td>30.0</td>\n      <td>97.0</td>\n      <td>0.9951</td>\n      <td>3.26</td>\n      <td>0.44</td>\n      <td>10.1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset class\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "\n",
    "    # Initialize the dataset class with the data\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/winequality-white.csv', skiprows=1, dtype=np.float32, delimiter=',')\n",
    "        self.x_data = torch.from_numpy(xy[:,0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:,-1])\n",
    "        self.y_data = self.y_data.long()\n",
    "        self.len = xy.shape[0]\n",
    "\n",
    "    # Return item from the tensor based on the index value\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # Return the length of the tensor\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset object based on the Mydataset class\n",
    "\n",
    "dataset = Mydataset()\n",
    "\n",
    "# Creating a loader file based on this dataset using the Dataloader utility\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=32, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model for classification\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    # Initialize the Mymodel class\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(11, 64)\n",
    "        self.l2 = torch.nn.Linear(64, 32)\n",
    "        self.l3 = torch.nn.Linear(32,16)\n",
    "        self.l4 = torch.nn.Linear(16,10)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = self.relu(self.l1(x))\n",
    "        out_2 = self.relu(self.l2(out_1))\n",
    "        out_3 = self.relu(self.l3(out_2))\n",
    "        y_pred = self.l4(out_3)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimus = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 0, | Loss: 1.756433367729187\nEpoch: 0, | Loss: 1.084341287612915\nEpoch: 0, | Loss: 1.2398041486740112\nEpoch: 0, | Loss: 1.2816483974456787\nEpoch: 0, | Loss: 1.1007722616195679\nEpoch: 0, | Loss: 1.3398348093032837\nEpoch: 0, | Loss: 1.2524685859680176\nEpoch: 0, | Loss: 1.2639210224151611\nEpoch: 0, | Loss: 1.282962441444397\nEpoch: 0, | Loss: 1.1008296012878418\nEpoch: 0, | Loss: 1.2006319761276245\nEpoch: 0, | Loss: 1.3502143621444702\nEpoch: 0, | Loss: 1.1859476566314697\nEpoch: 0, | Loss: 1.4054738283157349\nEpoch: 0, | Loss: 1.1343977451324463\nEpoch: 0, | Loss: 1.2453234195709229\nEpoch: 0, | Loss: 1.3625078201293945\nEpoch: 0, | Loss: 1.3253774642944336\nEpoch: 0, | Loss: 1.3961822986602783\nEpoch: 0, | Loss: 1.3755953311920166\nEpoch: 0, | Loss: 1.0844221115112305\nEpoch: 0, | Loss: 1.3290928602218628\nEpoch: 0, | Loss: 1.7520118951797485\nEpoch: 0, | Loss: 1.248341679573059\nEpoch: 0, | Loss: 1.0518062114715576\nEpoch: 0, | Loss: 1.2712175846099854\nEpoch: 0, | Loss: 1.447879672050476\nEpoch: 0, | Loss: 1.2612885236740112\nEpoch: 0, | Loss: 1.422073245048523\nEpoch: 0, | Loss: 1.5215140581130981\nEpoch: 0, | Loss: 1.3892000913619995\nEpoch: 0, | Loss: 1.1072907447814941\nEpoch: 0, | Loss: 1.2334656715393066\nEpoch: 0, | Loss: 1.1808644533157349\nEpoch: 0, | Loss: 1.6566005945205688\nEpoch: 0, | Loss: 1.2929452657699585\nEpoch: 0, | Loss: 1.4340423345565796\nEpoch: 0, | Loss: 1.218409776687622\nEpoch: 0, | Loss: 1.463149905204773\nEpoch: 0, | Loss: 1.213962197303772\nEpoch: 0, | Loss: 1.2909905910491943\nEpoch: 0, | Loss: 1.3370096683502197\nEpoch: 0, | Loss: 1.2478351593017578\nEpoch: 0, | Loss: 1.2110052108764648\nEpoch: 0, | Loss: 1.6760051250457764\nEpoch: 0, | Loss: 1.1968765258789062\nEpoch: 0, | Loss: 1.2450307607650757\nEpoch: 0, | Loss: 1.1623587608337402\nEpoch: 0, | Loss: 1.4190447330474854\nEpoch: 0, | Loss: 1.3626271486282349\nEpoch: 0, | Loss: 1.188270092010498\nEpoch: 0, | Loss: 1.3932428359985352\nEpoch: 0, | Loss: 1.2388060092926025\nEpoch: 0, | Loss: 1.3176240921020508\nEpoch: 0, | Loss: 1.3304051160812378\nEpoch: 0, | Loss: 1.2774662971496582\nEpoch: 0, | Loss: 1.3109029531478882\nEpoch: 0, | Loss: 1.4816749095916748\nEpoch: 0, | Loss: 1.3870813846588135\nEpoch: 0, | Loss: 1.2540956735610962\nEpoch: 0, | Loss: 1.309913158416748\nEpoch: 0, | Loss: 1.2840979099273682\nEpoch: 0, | Loss: 1.1009389162063599\nEpoch: 0, | Loss: 1.1775392293930054\nEpoch: 0, | Loss: 1.0523645877838135\nEpoch: 0, | Loss: 1.5078932046890259\nEpoch: 0, | Loss: 1.4824919700622559\nEpoch: 0, | Loss: 1.1031649112701416\nEpoch: 0, | Loss: 1.0372084379196167\nEpoch: 0, | Loss: 1.4036259651184082\nEpoch: 0, | Loss: 1.1167181730270386\nEpoch: 0, | Loss: 1.3177129030227661\nEpoch: 0, | Loss: 1.4202280044555664\nEpoch: 0, | Loss: 1.4587438106536865\nEpoch: 0, | Loss: 1.1930605173110962\nEpoch: 0, | Loss: 1.1292318105697632\nEpoch: 0, | Loss: 1.0617456436157227\nEpoch: 0, | Loss: 1.178981065750122\nEpoch: 0, | Loss: 1.2181856632232666\nEpoch: 0, | Loss: 1.2632627487182617\nEpoch: 0, | Loss: 1.1817893981933594\nEpoch: 0, | Loss: 1.1683629751205444\nEpoch: 0, | Loss: 1.611451506614685\nEpoch: 0, | Loss: 1.3743221759796143\nEpoch: 0, | Loss: 1.2293481826782227\nEpoch: 0, | Loss: 1.3875333070755005\nEpoch: 0, | Loss: 1.2539887428283691\nEpoch: 0, | Loss: 1.2650847434997559\nEpoch: 0, | Loss: 1.1299192905426025\nEpoch: 0, | Loss: 1.391156554222107\nEpoch: 0, | Loss: 1.300539493560791\nEpoch: 0, | Loss: 1.0793477296829224\nEpoch: 0, | Loss: 1.6700665950775146\nEpoch: 0, | Loss: 1.251083254814148\nEpoch: 0, | Loss: 1.311794400215149\nEpoch: 0, | Loss: 1.2355707883834839\nEpoch: 0, | Loss: 1.7282485961914062\nEpoch: 0, | Loss: 1.1501705646514893\nEpoch: 0, | Loss: 1.3597657680511475\nEpoch: 0, | Loss: 1.1651781797409058\nEpoch: 0, | Loss: 1.3110222816467285\nEpoch: 0, | Loss: 1.6248635053634644\nEpoch: 0, | Loss: 1.1450632810592651\nEpoch: 0, | Loss: 1.1833020448684692\nEpoch: 0, | Loss: 1.5490548610687256\nEpoch: 0, | Loss: 1.2332658767700195\nEpoch: 0, | Loss: 1.2717041969299316\nEpoch: 0, | Loss: 1.499459981918335\nEpoch: 0, | Loss: 1.227103352546692\nEpoch: 0, | Loss: 1.193637728691101\nEpoch: 0, | Loss: 1.244114637374878\nEpoch: 0, | Loss: 1.291919231414795\nEpoch: 0, | Loss: 1.2115942239761353\nEpoch: 0, | Loss: 1.4701423645019531\nEpoch: 0, | Loss: 1.4692375659942627\nEpoch: 0, | Loss: 1.1239627599716187\nEpoch: 0, | Loss: 1.3325225114822388\nEpoch: 0, | Loss: 1.3494784832000732\nEpoch: 0, | Loss: 1.2421330213546753\nEpoch: 0, | Loss: 1.5091298818588257\nEpoch: 0, | Loss: 1.297631025314331\nEpoch: 0, | Loss: 1.2244142293930054\nEpoch: 0, | Loss: 1.2834659814834595\nEpoch: 0, | Loss: 1.273996353149414\nEpoch: 0, | Loss: 1.4591169357299805\nEpoch: 0, | Loss: 1.183942437171936\nEpoch: 0, | Loss: 1.011520504951477\nEpoch: 0, | Loss: 1.2380011081695557\nEpoch: 0, | Loss: 1.4024224281311035\nEpoch: 0, | Loss: 1.139121174812317\nEpoch: 0, | Loss: 1.2269700765609741\nEpoch: 0, | Loss: 1.3007882833480835\nEpoch: 0, | Loss: 1.6031306982040405\nEpoch: 0, | Loss: 1.1335954666137695\nEpoch: 0, | Loss: 1.4247970581054688\nEpoch: 0, | Loss: 1.1235253810882568\nEpoch: 0, | Loss: 1.4010425806045532\nEpoch: 0, | Loss: 1.2210664749145508\nEpoch: 0, | Loss: 1.4678453207015991\nEpoch: 0, | Loss: 1.2090983390808105\nEpoch: 0, | Loss: 1.2411819696426392\nEpoch: 0, | Loss: 1.5199180841445923\nEpoch: 0, | Loss: 1.2131812572479248\nEpoch: 0, | Loss: 1.326535940170288\nEpoch: 0, | Loss: 1.6301542520523071\nEpoch: 0, | Loss: 1.3725135326385498\nEpoch: 0, | Loss: 1.2347798347473145\nEpoch: 0, | Loss: 1.1588331460952759\nEpoch: 0, | Loss: 1.1402033567428589\nEpoch: 0, | Loss: 1.3818988800048828\nEpoch: 0, | Loss: 1.558655023574829\nEpoch: 0, | Loss: 1.3235424757003784\nEpoch: 0, | Loss: 1.012758493423462\nEpoch: 0, | Loss: 1.4458292722702026\nEpoch: 1, | Loss: 1.1494163274765015\nEpoch: 1, | Loss: 1.5127333402633667\nEpoch: 1, | Loss: 1.0710644721984863\nEpoch: 1, | Loss: 1.325239896774292\nEpoch: 1, | Loss: 1.0804272890090942\nEpoch: 1, | Loss: 1.5886611938476562\nEpoch: 1, | Loss: 1.2305622100830078\nEpoch: 1, | Loss: 1.3778977394104004\nEpoch: 1, | Loss: 1.2961015701293945\nEpoch: 1, | Loss: 1.344267725944519\nEpoch: 1, | Loss: 1.2853035926818848\nEpoch: 1, | Loss: 1.462005615234375\nEpoch: 1, | Loss: 1.4919780492782593\nEpoch: 1, | Loss: 1.3722729682922363\nEpoch: 1, | Loss: 1.322454571723938\nEpoch: 1, | Loss: 1.5486449003219604\nEpoch: 1, | Loss: 1.1968246698379517\nEpoch: 1, | Loss: 1.1492953300476074\nEpoch: 1, | Loss: 1.1030819416046143\nEpoch: 1, | Loss: 1.3961551189422607\nEpoch: 1, | Loss: 1.2197537422180176\nEpoch: 1, | Loss: 1.3928762674331665\nEpoch: 1, | Loss: 1.307381272315979\nEpoch: 1, | Loss: 1.2360962629318237\nEpoch: 1, | Loss: 1.0710458755493164\nEpoch: 1, | Loss: 1.0596511363983154\nEpoch: 1, | Loss: 1.0853190422058105\nEpoch: 1, | Loss: 1.0965861082077026\nEpoch: 1, | Loss: 1.1520949602127075\nEpoch: 1, | Loss: 1.2259347438812256\nEpoch: 1, | Loss: 1.4113998413085938\nEpoch: 1, | Loss: 1.2449194192886353\nEpoch: 1, | Loss: 1.2817134857177734\nEpoch: 1, | Loss: 1.1083173751831055\nEpoch: 1, | Loss: 1.3955156803131104\nEpoch: 1, | Loss: 1.495963215827942\nEpoch: 1, | Loss: 1.423494815826416\nEpoch: 1, | Loss: 1.3785333633422852\nEpoch: 1, | Loss: 1.2990778684616089\nEpoch: 1, | Loss: 1.2252511978149414\nEpoch: 1, | Loss: 1.3543459177017212\nEpoch: 1, | Loss: 1.336092233657837\nEpoch: 1, | Loss: 1.394531011581421\nEpoch: 1, | Loss: 1.2090952396392822\nEpoch: 1, | Loss: 1.3966002464294434\nEpoch: 1, | Loss: 1.1067705154418945\nEpoch: 1, | Loss: 1.2832252979278564\nEpoch: 1, | Loss: 1.4224506616592407\nEpoch: 1, | Loss: 1.4573304653167725\nEpoch: 1, | Loss: 1.2919801473617554\nEpoch: 1, | Loss: 1.3525831699371338\nEpoch: 1, | Loss: 1.113459825515747\nEpoch: 1, | Loss: 1.4404045343399048\nEpoch: 1, | Loss: 1.3068095445632935\nEpoch: 1, | Loss: 1.2566174268722534\nEpoch: 1, | Loss: 1.1975817680358887\nEpoch: 1, | Loss: 1.2946299314498901\nEpoch: 1, | Loss: 1.5031222105026245\nEpoch: 1, | Loss: 1.4052891731262207\nEpoch: 1, | Loss: 1.3215574026107788\nEpoch: 1, | Loss: 1.502802848815918\nEpoch: 1, | Loss: 1.2137442827224731\nEpoch: 1, | Loss: 1.2630358934402466\nEpoch: 1, | Loss: 1.2480052709579468\nEpoch: 1, | Loss: 1.1748147010803223\nEpoch: 1, | Loss: 1.3075768947601318\nEpoch: 1, | Loss: 1.4848074913024902\nEpoch: 1, | Loss: 1.359654188156128\nEpoch: 1, | Loss: 1.367064356803894\nEpoch: 1, | Loss: 1.3566868305206299\nEpoch: 1, | Loss: 1.1814059019088745\nEpoch: 1, | Loss: 1.382716178894043\nEpoch: 1, | Loss: 1.2418136596679688\nEpoch: 1, | Loss: 1.487998127937317\nEpoch: 1, | Loss: 1.1606115102767944\nEpoch: 1, | Loss: 1.210985541343689\nEpoch: 1, | Loss: 1.5726450681686401\nEpoch: 1, | Loss: 1.1975990533828735\nEpoch: 1, | Loss: 1.2851539850234985\nEpoch: 1, | Loss: 1.2811022996902466\nEpoch: 1, | Loss: 0.9745256304740906\nEpoch: 1, | Loss: 1.3818031549453735\nEpoch: 1, | Loss: 1.266993522644043\nEpoch: 1, | Loss: 1.281699776649475\nEpoch: 1, | Loss: 1.356113314628601\nEpoch: 1, | Loss: 1.031165361404419\nEpoch: 1, | Loss: 1.1052879095077515\nEpoch: 1, | Loss: 1.2047781944274902\nEpoch: 1, | Loss: 1.488179326057434\nEpoch: 1, | Loss: 1.4291750192642212\nEpoch: 1, | Loss: 1.198131799697876\nEpoch: 1, | Loss: 1.1763678789138794\nEpoch: 1, | Loss: 1.0646899938583374\nEpoch: 1, | Loss: 1.289359450340271\nEpoch: 1, | Loss: 1.4328956604003906\nEpoch: 1, | Loss: 1.1799676418304443\nEpoch: 1, | Loss: 1.3820618391036987\nEpoch: 1, | Loss: 1.1124380826950073\nEpoch: 1, | Loss: 1.1585581302642822\nEpoch: 1, | Loss: 1.2595069408416748\nEpoch: 1, | Loss: 1.4869842529296875\nEpoch: 1, | Loss: 1.3511043787002563\nEpoch: 1, | Loss: 1.302904486656189\nEpoch: 1, | Loss: 1.2984356880187988\nEpoch: 1, | Loss: 1.268978476524353\nEpoch: 1, | Loss: 1.3513026237487793\nEpoch: 1, | Loss: 1.346780776977539\nEpoch: 1, | Loss: 1.4619845151901245\nEpoch: 1, | Loss: 1.3158633708953857\nEpoch: 1, | Loss: 1.3570668697357178\nEpoch: 1, | Loss: 1.3115622997283936\nEpoch: 1, | Loss: 1.1120936870574951\nEpoch: 1, | Loss: 1.1535944938659668\nEpoch: 1, | Loss: 1.1250364780426025\nEpoch: 1, | Loss: 1.2397269010543823\nEpoch: 1, | Loss: 1.330161213874817\nEpoch: 1, | Loss: 1.4579788446426392\nEpoch: 1, | Loss: 1.153367280960083\nEpoch: 1, | Loss: 1.1978938579559326\nEpoch: 1, | Loss: 1.2818348407745361\nEpoch: 1, | Loss: 1.240523099899292\nEpoch: 1, | Loss: 1.283247470855713\nEpoch: 1, | Loss: 1.3392599821090698\nEpoch: 1, | Loss: 1.2881269454956055\nEpoch: 1, | Loss: 1.287299394607544\nEpoch: 1, | Loss: 1.2496486902236938\nEpoch: 1, | Loss: 1.3451167345046997\nEpoch: 1, | Loss: 1.514665961265564\nEpoch: 1, | Loss: 1.3301310539245605\nEpoch: 1, | Loss: 1.2946044206619263\nEpoch: 1, | Loss: 1.3693774938583374\nEpoch: 1, | Loss: 1.2106473445892334\nEpoch: 1, | Loss: 1.2141366004943848\nEpoch: 1, | Loss: 1.5447413921356201\nEpoch: 1, | Loss: 1.386259913444519\nEpoch: 1, | Loss: 1.4681321382522583\nEpoch: 1, | Loss: 1.4315094947814941\nEpoch: 1, | Loss: 1.389166235923767\nEpoch: 1, | Loss: 1.1746869087219238\nEpoch: 1, | Loss: 1.282170295715332\nEpoch: 1, | Loss: 1.3671268224716187\nEpoch: 1, | Loss: 1.6811981201171875\nEpoch: 1, | Loss: 1.1945055723190308\nEpoch: 1, | Loss: 1.1401394605636597\nEpoch: 1, | Loss: 1.3543992042541504\nEpoch: 1, | Loss: 1.3394800424575806\nEpoch: 1, | Loss: 1.4498770236968994\nEpoch: 1, | Loss: 1.2716094255447388\nEpoch: 1, | Loss: 1.3174337148666382\nEpoch: 1, | Loss: 1.1227093935012817\nEpoch: 1, | Loss: 1.2315298318862915\nEpoch: 1, | Loss: 1.375878095626831\nEpoch: 1, | Loss: 1.3357716798782349\nEpoch: 1, | Loss: 1.0040476322174072\n"
    }
   ],
   "source": [
    "# Creating the training loop and printing the loss. For this experiment we are using the whole dataset as training. \n",
    "\n",
    "for epoch in range(2):\n",
    "    for _, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        # Forward pass in the network to get the prediction\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Calcuate the loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "\n",
    "        # Print the result\n",
    "        print(f'Epoch: {epoch}, | Loss: {loss}')\n",
    "\n",
    "        # Backpropagation and updating the weights\n",
    "        optimus.zero_grad()\n",
    "        loss.backward()\n",
    "        optimus.step()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see the result is shit. To be further improved in lecture 9"
   ]
  }
 ]
}